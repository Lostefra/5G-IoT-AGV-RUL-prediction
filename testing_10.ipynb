{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0d267c5c",
      "metadata": {
        "id": "0d267c5c"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxFZj0eboNY-"
      },
      "outputs": [],
      "source": [
        "# Prediction margin: the only parameter to set. Recommended: margin in {5, 10} (aka 0.5, 1 second)\n",
        "margin = 10"
      ],
      "id": "dxFZj0eboNY-"
    },
    {
      "cell_type": "markdown",
      "id": "3df02c22",
      "metadata": {
        "id": "3df02c22"
      },
      "source": [
        "## Import libraries and define utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3375203",
      "metadata": {
        "id": "d3375203"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import random\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from math import floor\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c34a05dd",
      "metadata": {
        "id": "c34a05dd"
      },
      "outputs": [],
      "source": [
        "mean = lambda l: sum(l) / len(l)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5b3b931",
      "metadata": {
        "id": "a5b3b931"
      },
      "source": [
        "#### Set up global variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0d48c12",
      "metadata": {
        "id": "e0d48c12"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.float_format', lambda x: '%.4f' % x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95L0XnmxoNZH"
      },
      "outputs": [],
      "source": [
        "models_path = \"models_\" + str(margin) + \"/\"\n",
        "results_path = \"results_\" + str(margin) + \"/\"\n",
        "threshold_path = \"threshold_\" + str(margin) + \"/\""
      ],
      "id": "95L0XnmxoNZH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9k7OJDCoNZI"
      },
      "outputs": [],
      "source": [
        "w5_features_no_diff = [\n",
        " 'Gz_mean_w5',\n",
        " 'Ax_mean_w5',\n",
        " 'Ay_mean_w5',\n",
        " 'Gz_std_w5',\n",
        " 'Ax_std_w5',\n",
        " 'Ay_std_w5',\n",
        " 'Gz_min_w5',\n",
        " 'Ax_min_w5',\n",
        " 'Ay_min_w5',\n",
        " 'Gz_max_w5',\n",
        " 'Ax_max_w5',\n",
        " 'Ay_max_w5'\n",
        "]\n",
        "\n",
        "w5_features_diff = [\n",
        " 'differencing_Gz_mean_w5',\n",
        " 'differencing_Ax_mean_w5',\n",
        " 'differencing_Ay_mean_w5',\n",
        " 'differencing_Gz_std_w5',\n",
        " 'differencing_Ax_std_w5',\n",
        " 'differencing_Ay_std_w5',\n",
        " 'differencing_Gz_min_w5',\n",
        " 'differencing_Ax_min_w5',\n",
        " 'differencing_Ay_min_w5',\n",
        " 'differencing_Gz_max_w5',\n",
        " 'differencing_Ax_max_w5',\n",
        " 'differencing_Ay_max_w5',\n",
        "]\n",
        "\n",
        "w10_features_no_diff = [\n",
        " 'Gz_mean_w10',\n",
        " 'Ax_mean_w10',\n",
        " 'Ay_mean_w10',\n",
        " 'Gz_std_w10',\n",
        " 'Ax_std_w10',\n",
        " 'Ay_std_w10',\n",
        " 'Gz_min_w10',\n",
        " 'Ax_min_w10',\n",
        " 'Ay_min_w10',\n",
        " 'Gz_max_w10',\n",
        " 'Ax_max_w10',\n",
        " 'Ay_max_w10'\n",
        "]\n",
        "\n",
        "w10_features_diff = [\n",
        " 'differencing_Gz_mean_w10',\n",
        " 'differencing_Ax_mean_w10',\n",
        " 'differencing_Ay_mean_w10',\n",
        " 'differencing_Gz_std_w10',\n",
        " 'differencing_Ax_std_w10',\n",
        " 'differencing_Ay_std_w10',\n",
        " 'differencing_Gz_min_w10',\n",
        " 'differencing_Ax_min_w10',\n",
        " 'differencing_Ay_min_w10',\n",
        " 'differencing_Gz_max_w10',\n",
        " 'differencing_Ax_max_w10',\n",
        " 'differencing_Ay_max_w10'\n",
        "]\n",
        "\n",
        "w15_features_no_diff = [\n",
        " 'Gz_mean_w15',\n",
        " 'Ax_mean_w15',\n",
        " 'Ay_mean_w15',\n",
        " 'Gz_std_w15',\n",
        " 'Ax_std_w15',\n",
        " 'Ay_std_w15',\n",
        " 'Gz_min_w15',\n",
        " 'Ax_min_w15',\n",
        " 'Ay_min_w15',\n",
        " 'Gz_max_w15',\n",
        " 'Ax_max_w15',\n",
        " 'Ay_max_w15'\n",
        "]\n",
        "\n",
        "w15_features_diff = [\n",
        " 'differencing_Gz_mean_w15',\n",
        " 'differencing_Ax_mean_w15',\n",
        " 'differencing_Ay_mean_w15',\n",
        " 'differencing_Gz_std_w15',\n",
        " 'differencing_Ax_std_w15',\n",
        " 'differencing_Ay_std_w15',\n",
        " 'differencing_Gz_min_w15',\n",
        " 'differencing_Ax_min_w15',\n",
        " 'differencing_Ay_min_w15',\n",
        " 'differencing_Gz_max_w15',\n",
        " 'differencing_Ax_max_w15',\n",
        " 'differencing_Ay_max_w15'\n",
        "]\n",
        "\n",
        "w20_features_no_diff = [\n",
        " 'Gz_mean_w20',\n",
        " 'Ax_mean_w20',\n",
        " 'Ay_mean_w20',\n",
        " 'Gz_std_w20',\n",
        " 'Ax_std_w20',\n",
        " 'Ay_std_w20',\n",
        " 'Gz_min_w20',\n",
        " 'Ax_min_w20',\n",
        " 'Ay_min_w20',\n",
        " 'Gz_max_w20',\n",
        " 'Ax_max_w20',\n",
        " 'Ay_max_w20'\n",
        "]\n",
        "\n",
        "w20_features_diff = [\n",
        " 'differencing_Gz_mean_w20',\n",
        " 'differencing_Ax_mean_w20',\n",
        " 'differencing_Ay_mean_w20',\n",
        " 'differencing_Gz_std_w20',\n",
        " 'differencing_Ax_std_w20',\n",
        " 'differencing_Ay_std_w20',\n",
        " 'differencing_Gz_min_w20',\n",
        " 'differencing_Ax_min_w20',\n",
        " 'differencing_Ay_min_w20',\n",
        " 'differencing_Gz_max_w20',\n",
        " 'differencing_Ax_max_w20',\n",
        " 'differencing_Ay_max_w20',\n",
        "]\n",
        "\n",
        "features = {\n",
        "    \"all_features\": w5_features_no_diff + w10_features_no_diff + w15_features_no_diff + w20_features_no_diff + w5_features_diff + w10_features_diff + w15_features_diff + w20_features_diff + ['label'],\n",
        "    \"w5_features\": w5_features_no_diff + w5_features_diff + ['label'],\n",
        "    \"w10_features\": w10_features_no_diff + w10_features_diff + ['label'],\n",
        "    \"w15_features\": w15_features_no_diff + w15_features_diff + ['label'],\n",
        "    \"w20_features\": w20_features_no_diff + w20_features_diff + ['label'],\n",
        "    \"no_diff_features\": w5_features_no_diff + w10_features_no_diff + w15_features_no_diff + w20_features_no_diff + ['label'],\n",
        "    \"diff_features\": w5_features_diff + w10_features_diff + w15_features_diff + w20_features_diff + ['label']\n",
        "}"
      ],
      "id": "y9k7OJDCoNZI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7TWzszdoNZJ"
      },
      "source": [
        "#### Data import"
      ],
      "id": "p7TWzszdoNZJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUXUFRuQoNZK"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"5G_IIoT_RUL_dataset/test/test_\" + str(margin) + \".csv\", index_col=[0])"
      ],
      "id": "QUXUFRuQoNZK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edf52a77",
      "metadata": {
        "id": "edf52a77"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klwlE0j3oNZa"
      },
      "source": [
        "## Remaining useful life (RUL)\n",
        "\n",
        "The following plot shows the Remaining Useful Life (RUL), namely the number of time steps before that a failure occurs."
      ],
      "id": "klwlE0j3oNZa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKyLgDe8oNZa"
      },
      "outputs": [],
      "source": [
        "plt.plot(df[\"label\"])\n",
        "plt.show()"
      ],
      "id": "eKyLgDe8oNZa"
    },
    {
      "cell_type": "markdown",
      "id": "e5765f47",
      "metadata": {
        "id": "e5765f47"
      },
      "source": [
        "# Machine learning: testing phase"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d279553f",
      "metadata": {
        "id": "d279553f"
      },
      "source": [
        "## Dataset preprocessing for machine learning models\n",
        "\n",
        "In this section, RUL labels are converted to binary labels (`0/1`, namely `not_fault/fault`) in order to perform classification instead of regression.\n",
        "\n",
        "For the `AutoEncoder` model, the dataset is partitioned such that the training set does not contain faults or samples which anticipate a fault. In other words, each sample must be compliant with the `good_samples_thr` threshold.\n",
        "\n",
        "We basically need an entire section of dataset where faults are not present."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d19cf5c1",
      "metadata": {
        "id": "d19cf5c1"
      },
      "outputs": [],
      "source": [
        "def build_dataset_for_ml_model(df, training_columns, split_size=0.75, as_list=False, ae=False):\n",
        "    dfs = []\n",
        "    df_main = df[training_columns]\n",
        "    fault_indexes = df_main.index[df_main[\"label\"] == 0].tolist() # list of indexes representing faults\n",
        "    good_samples_thr = margin * 2\n",
        "\n",
        "    previous = 0\n",
        "    for fi in fault_indexes:\n",
        "        dfs.append(df_main.iloc[previous:fi+1, :])\n",
        "        previous = fi + 1\n",
        "\n",
        "    rnd_list = list(range(len(dfs)))\n",
        "\n",
        "    # If split_size is 1, there will be no val/test set\n",
        "    train_size = floor(len(dfs) * split_size)\n",
        "    train_index = rnd_list[:train_size]\n",
        "    test_index = rnd_list[train_size:]\n",
        "    train_rul = []\n",
        "    test_rul = []\n",
        "\n",
        "    if not as_list:\n",
        "        first = True\n",
        "        for ti in train_index:\n",
        "            if not ae:\n",
        "                to_concat = dfs[ti].copy()\n",
        "            else:\n",
        "                to_concat = dfs[ti][dfs[ti][\"label\"] >= good_samples_thr].copy()\n",
        "            if first:\n",
        "                training_set = to_concat\n",
        "                first = False\n",
        "            else:\n",
        "                training_set = pd.concat([training_set, to_concat])\n",
        "\n",
        "        first = True\n",
        "        for ti in test_index:\n",
        "            to_concat = dfs[ti].copy()\n",
        "            if first:\n",
        "                test_set = to_concat\n",
        "                first = False\n",
        "            else:\n",
        "                test_set = pd.concat([test_set, to_concat])\n",
        "\n",
        "        train_rul = training_set['label'].tolist()\n",
        "        if split_size < 1:\n",
        "            test_rul = test_set['label'].tolist()\n",
        "\n",
        "        training_set['label'] = (training_set['label'] >= margin).map({True: 1, False: 0})\n",
        "        if split_size < 1:\n",
        "            test_set['label'] = (test_set['label'] >= margin).map({True: 1, False: 0})\n",
        "\n",
        "        training_set = training_set.to_numpy()\n",
        "        if split_size < 1:\n",
        "            test_set = test_set.to_numpy()\n",
        "\n",
        "    else:\n",
        "        first = True\n",
        "        for ti in train_index:\n",
        "            if not ae:\n",
        "                to_concat = dfs[ti].copy()\n",
        "            else:\n",
        "                to_concat = dfs[ti][dfs[ti][\"label\"] >= good_samples_thr].copy()\n",
        "            if first:\n",
        "                training_set = [to_concat]\n",
        "                first = False\n",
        "            else:\n",
        "                training_set.append(to_concat)\n",
        "\n",
        "        first = True\n",
        "        for ti in test_index:\n",
        "            to_concat = dfs[ti].copy()\n",
        "            if first:\n",
        "                test_set = [to_concat]\n",
        "                first = False\n",
        "            else:\n",
        "                test_set.append(to_concat)\n",
        "\n",
        "        for t in training_set:\n",
        "            train_rul = train_rul + t['label'].tolist()\n",
        "            t['label'] = (t['label'] >= margin).map({True: 1, False: 0})\n",
        "        if split_size < 1:\n",
        "            for t in test_set:\n",
        "                test_rul = test_rul + t['label'].tolist()\n",
        "                t['label'] = (t['label'] >= margin).map({True: 1, False: 0})\n",
        "    if split_size < 1:\n",
        "        return training_set, test_set\n",
        "    return training_set"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c131072",
      "metadata": {
        "id": "0c131072"
      },
      "source": [
        "## Cost model for threshold optimization and performance evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d58986c8",
      "metadata": {
        "id": "d58986c8"
      },
      "outputs": [],
      "source": [
        "all_perf = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7551194",
      "metadata": {
        "id": "f7551194"
      },
      "outputs": [],
      "source": [
        "BASE_FP = 0.2\n",
        "BASE_FN = 1\n",
        "\n",
        "def false_positive_cost(i, is_fault, fault_found):\n",
        "    return BASE_FP\n",
        "\n",
        "def false_negative_cost(i, is_fault, fault_found):\n",
        "    if not fault_found:\n",
        "        for j in range(1, margin + 1):\n",
        "            if i + j < is_fault.shape[0] and not is_fault[i + j] or i + j >= is_fault.shape[0]:\n",
        "                return (margin + 1 - j) * BASE_FN\n",
        "    else:\n",
        "        return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1359dec",
      "metadata": {
        "id": "c1359dec"
      },
      "outputs": [],
      "source": [
        "def threshold_optimization(signal, rul, start, end, n_steps):\n",
        "    best_cost = sys.maxsize\n",
        "    best_thr = -1\n",
        "    all_cost = []\n",
        "    all_thr = []\n",
        "    is_fault = (rul == 0)\n",
        "\n",
        "    for thr in np.linspace(start, end, n_steps):\n",
        "        tmp_cost = 0\n",
        "        fault_found = False\n",
        "        for i in range(signal.shape[0]):\n",
        "            if is_fault[i] and signal[i] >= thr:\n",
        "                fault_found = True\n",
        "            if not is_fault[i]:\n",
        "                fault_found = False\n",
        "            if not is_fault[i] and signal[i] >= thr:\n",
        "                tmp_cost += false_positive_cost(i, is_fault, fault_found)\n",
        "            elif is_fault[i] and signal[i] <= thr:\n",
        "                tmp_cost += false_negative_cost(i, is_fault, fault_found)\n",
        "        if tmp_cost < best_cost:\n",
        "            best_thr = thr\n",
        "            best_cost = tmp_cost\n",
        "        all_cost.append(tmp_cost)\n",
        "        all_thr.append(thr)\n",
        "\n",
        "    return best_cost, best_thr, all_cost, all_thr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14a8cce1",
      "metadata": {
        "id": "14a8cce1"
      },
      "outputs": [],
      "source": [
        "def performance_evaluation(signal, thr, rul):\n",
        "    fp, fn, tp, tot_p = 0, 0, 0, 0\n",
        "    cost = 0\n",
        "    alarm = (signal >= thr)\n",
        "    anticipation = []\n",
        "    is_fault = (rul == 0)\n",
        "\n",
        "    fault_found = False\n",
        "    for i in range(len(rul)):\n",
        "        if i > 0 and is_fault[i] and not is_fault[i - 1]:\n",
        "            tot_p += 1\n",
        "            start = i\n",
        "        if is_fault[i] and not fault_found and alarm[i]:\n",
        "            tp += 1\n",
        "            fault_found = True\n",
        "            anticipation.append((margin - 1) - (i - start))\n",
        "        if (i < len(rul) - 1 and is_fault[i] and not is_fault[i + 1] and not fault_found) or (i == len(rul) - 1 and not fault_found):\n",
        "            fn += 1\n",
        "        if is_fault[i] and signal[i] <= thr:\n",
        "            cost += false_negative_cost(i, is_fault, fault_found)\n",
        "        if not is_fault[i]:\n",
        "            fault_found = False\n",
        "            if alarm[i]:\n",
        "                fp += 1\n",
        "                cost += false_positive_cost(i, is_fault, fault_found)\n",
        "\n",
        "    tot_a = sum(anticipation) / 10\n",
        "    if sum(anticipation) > 0:\n",
        "        mean_a = mean(anticipation) / 10\n",
        "    else:\n",
        "        mean_a = 0\n",
        "\n",
        "    return [cost, mean_a, tp, fn, fp]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89ec634d",
      "metadata": {
        "id": "89ec634d"
      },
      "source": [
        "## Baseline: raw signal pre-anomaly detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LiKF4P1zoNZg"
      },
      "outputs": [],
      "source": [
        "seed = 100\n",
        "feature = \"Ax_diff\"\n",
        "\n",
        "f = results_path + \"raw_signal-\" + feature + \"-s\" + str(seed) + \"-p0\"\n",
        "with open(f, \"rb\") as file:\n",
        "    _, _, _, best_cost_raw, best_thr_raw, _, _, _ = pickle.load(file)\n",
        "\n",
        "training_columns = [feature, \"label\"]\n",
        "test_set_raw = build_dataset_for_ml_model(df, training_columns=training_columns, split_size=1)\n",
        "\n",
        "test_raw_signal, test_raw_rul = -test_set_raw[:, 0], test_set_raw[:, -1]\n",
        "\n",
        "t = threshold_path + \"raw_signal-\" + feature + \"-s\" + str(seed) + \"-p0\"\n",
        "to_serialize = test_raw_signal, best_thr_raw, test_raw_rul\n",
        "with open(t, \"wb\") as file:\n",
        "    pickle.dump(to_serialize, file)\n",
        "\n",
        "test_perf_raw = performance_evaluation(test_raw_signal, best_thr_raw, test_raw_rul)\n",
        "all_perf.append([\"raw_signal\", seed, feature, {}, \"0\"] + test_perf_raw)"
      ],
      "id": "LiKF4P1zoNZg"
    },
    {
      "cell_type": "markdown",
      "id": "c581b70d",
      "metadata": {
        "id": "c581b70d"
      },
      "source": [
        "## Autoencoders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "ozH9YJyfoNZg"
      },
      "outputs": [],
      "source": [
        "params_ae = [{\"hidden_ae\": [16, 8, 2, 8, 16]},\n",
        "             {\"hidden_ae\": [64, 24, 9, 24, 64]},\n",
        "             {\"hidden_ae\": [128, 56, 18, 56, 128]}]\n",
        "\n",
        "seed = 200\n",
        "columns = \"w5_features\"\n",
        "params_idx = 2\n",
        "params = params_ae[params_idx]\n",
        "\n",
        "ae = keras.models.load_model(models_path + \"autoencoder\" + \"-\" + columns + \"-s\" + str(seed) + \"-p\" + str(params_idx))\n",
        "f = results_path + \"autoencoder-\" + columns + \"-s\" + str(seed) + \"-p\" + str(params_idx)\n",
        "with open(f, \"rb\") as file:\n",
        "    training_columns, _, _, best_cost_ae, best_thr_ae, _, _, _ = pickle.load(file)\n",
        "\n",
        "test_set_ae = build_dataset_for_ml_model(df, training_columns=training_columns, split_size=1)\n",
        "\n",
        "test_preds_ae = ae.predict(test_set_ae[:, :-1])\n",
        "\n",
        "test_signal_ae = pd.Series(data=np.sum(np.square(test_preds_ae - test_set_ae[:, :-1]), axis=1))\n",
        "test_rul_ae = test_set_ae[:, -1]\n",
        "\n",
        "t = threshold_path + \"autoencoder-\" + columns + \"-s\" + str(seed) + \"-p\" + str(params_idx)\n",
        "to_serialize = test_signal_ae, best_thr_ae, test_rul_ae\n",
        "with open(t, \"wb\") as file:\n",
        "    pickle.dump(to_serialize, file)\n",
        "\n",
        "test_perf_ae = performance_evaluation(test_signal_ae, best_thr_ae, test_rul_ae)\n",
        "all_perf.append([\"autoencoder\", seed, columns, params, params_idx] + test_perf_ae)"
      ],
      "id": "ozH9YJyfoNZg"
    },
    {
      "cell_type": "markdown",
      "id": "6dafa821",
      "metadata": {
        "id": "6dafa821"
      },
      "source": [
        "## Dense Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZstxiMsoNZh"
      },
      "outputs": [],
      "source": [
        "params_mlp = [{\"hidden_mlp\": []},\n",
        "              {\"hidden_mlp\": [32]},\n",
        "              {\"hidden_mlp\": [64, 32]},\n",
        "              {\"hidden_mlp\": [128, 64, 32]}]\n",
        "\n",
        "seed = 2800\n",
        "columns = \"w5_features\"\n",
        "params_idx = 3\n",
        "params = params_mlp[params_idx]\n",
        "\n",
        "mlp = keras.models.load_model(models_path + \"mlp\" + \"-\" + columns + \"-s\" + str(seed) + \"-p\" + str(params_idx))\n",
        "f = results_path + \"mlp-\" + columns + \"-s\" + str(seed) + \"-p\" + str(params_idx)\n",
        "with open(f, \"rb\") as file:\n",
        "    training_columns, _, _, best_cost_mlp, best_thr_mlp, _, _, _ = pickle.load(file)\n",
        "\n",
        "test_set_mlp = build_dataset_for_ml_model(df, training_columns=training_columns, split_size=1)\n",
        "\n",
        "test_preds_mlp = mlp.predict(test_set_mlp[:, :-1]).ravel()\n",
        "\n",
        "test_signal_mlp = pd.Series(data=(1 - test_preds_mlp))\n",
        "test_rul_mlp = test_set_mlp[:, -1]\n",
        "\n",
        "t = threshold_path + \"mlp-\" + columns + \"-s\" + str(seed) + \"-p\" + str(params_idx)\n",
        "to_serialize = test_signal_mlp, best_thr_mlp, test_rul_mlp\n",
        "with open(t, \"wb\") as file:\n",
        "    pickle.dump(to_serialize, file)\n",
        "\n",
        "test_perf_mlp = performance_evaluation(test_signal_mlp, best_thr_mlp, test_rul_mlp)\n",
        "all_perf.append([\"mlp\", seed, columns, params, params_idx] + test_perf_mlp)"
      ],
      "id": "eZstxiMsoNZh"
    },
    {
      "cell_type": "markdown",
      "id": "c8653164",
      "metadata": {
        "id": "c8653164"
      },
      "source": [
        "## Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nR7t1yAoNZh"
      },
      "outputs": [],
      "source": [
        "def sliding_window_2D(data, w_len, stride=1):\n",
        "    # Get shifted tables\n",
        "    m = len(data)\n",
        "    lt = [data.iloc[i:m-w_len+i+1:stride, :].values for i in range(w_len)]\n",
        "    # Reshape to add a new axis\n",
        "    s = lt[0].shape\n",
        "    for i in range(w_len):\n",
        "        lt[i] = lt[i].reshape(s[0], 1, s[1])\n",
        "    # Concatenate\n",
        "    wdata = np.concatenate(lt, axis=1)\n",
        "    return wdata\n",
        "\n",
        "\n",
        "def sliding_window_by_fault(data, cols, w_len, stride=1):\n",
        "    l_w, l_r = [], []\n",
        "    cols.pop()  # remove \"label\"\n",
        "    for gdata in data:\n",
        "        # Apply a sliding window\n",
        "        tmp_w = sliding_window_2D(gdata[cols], w_len, stride)\n",
        "        # Build the RUL vector\n",
        "        tmp_r = gdata['label'].iloc[w_len-1::stride]\n",
        "        # Store everything\n",
        "        l_w.append(tmp_w)\n",
        "        l_r.append(tmp_r)\n",
        "    res_w = np.concatenate(l_w)\n",
        "    res_r = np.concatenate(l_r)\n",
        "    return res_w, res_r"
      ],
      "id": "2nR7t1yAoNZh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWcnrHbAoNZh"
      },
      "outputs": [],
      "source": [
        "params_cnn = [{\"filters\": 1, \"kernel_size\": 3, \"hidden\": [32], \"w_len\": 5},\n",
        "              {\"filters\": 4, \"kernel_size\": 3, \"hidden\": [32], \"w_len\": 5},\n",
        "              {\"filters\": 1, \"kernel_size\": 5, \"hidden\": [32], \"w_len\": 5},\n",
        "              {\"filters\": 4, \"kernel_size\": 5, \"hidden\": [32], \"w_len\": 5},\n",
        "              {\"filters\": 4, \"kernel_size\": 5, \"hidden\": [64, 32], \"w_len\": 5},\n",
        "              {\"filters\": 1, \"kernel_size\": 3, \"hidden\": [32], \"w_len\": 10},\n",
        "              {\"filters\": 4, \"kernel_size\": 3, \"hidden\": [32], \"w_len\": 10},\n",
        "              {\"filters\": 1, \"kernel_size\": 5, \"hidden\": [32], \"w_len\": 10},\n",
        "              {\"filters\": 4, \"kernel_size\": 5, \"hidden\": [32], \"w_len\": 10},\n",
        "              {\"filters\": 4, \"kernel_size\": 5, \"hidden\": [64, 32], \"w_len\": 10},\n",
        "              {\"filters\": 4, \"kernel_size\": 7, \"hidden\": [128, 64, 32], \"w_len\": 10},\n",
        "              {\"filters\": 1, \"kernel_size\": 3, \"hidden\": [64, 32], \"w_len\": 5},\n",
        "              {\"filters\": 1, \"kernel_size\": 5, \"hidden\": [64, 32], \"w_len\": 5},\n",
        "              {\"filters\": 1, \"kernel_size\": 3, \"hidden\": [64, 32], \"w_len\": 10},\n",
        "              {\"filters\": 1, \"kernel_size\": 5, \"hidden\": [64, 32], \"w_len\": 10}]"
      ],
      "id": "YWcnrHbAoNZh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkAZn8xioNZi"
      },
      "outputs": [],
      "source": [
        "seed = 2200\n",
        "columns = \"all_features\"\n",
        "params_idx = 11\n",
        "params = params_cnn[params_idx]\n",
        "\n",
        "cnn = keras.models.load_model(models_path + \"conv_nn\" + \"-\" + columns + \"-s\" + str(seed) + \"-p\" + str(params_idx))\n",
        "f = results_path + \"conv_nn-\" + columns + \"-s\" + str(seed) + \"-p\" + str(params_idx)\n",
        "with open(f, \"rb\") as file:\n",
        "    training_columns, _, _, best_cost_cnn, best_thr_cnn, _, _, _ = pickle.load(file)\n",
        "\n",
        "test_set_cnn = build_dataset_for_ml_model(df, training_columns=training_columns, split_size=1, as_list=True)\n",
        "ts_sw, ts_sw_r = sliding_window_by_fault(test_set_cnn, training_columns.copy(), params[\"w_len\"])\n",
        "\n",
        "test_preds_cnn = cnn.predict(ts_sw).ravel()\n",
        "\n",
        "test_signal_cnn = pd.Series(data=(1 - test_preds_cnn))\n",
        "test_rul_cnn = ts_sw_r\n",
        "\n",
        "t = threshold_path + \"conv_nn-\" + columns + \"-s\" + str(seed) + \"-p\" + str(params_idx)\n",
        "to_serialize = test_signal_cnn, best_thr_cnn, test_rul_cnn\n",
        "with open(t, \"wb\") as file:\n",
        "    pickle.dump(to_serialize, file)\n",
        "\n",
        "test_perf_cnn = performance_evaluation(test_signal_cnn, best_thr_cnn, test_rul_cnn)\n",
        "all_perf.append([\"conv_nn\", seed, columns, params, params_idx] + test_perf_cnn)"
      ],
      "id": "hkAZn8xioNZi"
    },
    {
      "cell_type": "markdown",
      "id": "2f7eb87d",
      "metadata": {
        "id": "2f7eb87d"
      },
      "source": [
        "## Recurrent Neural Network (LSTM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcrlCRkNoNZj"
      },
      "outputs": [],
      "source": [
        "def create_dataset_3D(X, y, time_steps):\n",
        "    Xs, ys = [], []\n",
        "    for i in range(len(X)-time_steps):\n",
        "        v = X[i:i+time_steps, :]\n",
        "        Xs.append(v)\n",
        "        ys.append(y[i+time_steps])\n",
        "    return np.array(Xs), np.array(ys).reshape(-1, 1)"
      ],
      "id": "LcrlCRkNoNZj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3O-VlqD0oNZj"
      },
      "outputs": [],
      "source": [
        "params_rnn = [{\"time_steps\": 5, \"units\": 64},\n",
        "              {\"time_steps\": 5, \"units\": 128},\n",
        "              {\"time_steps\": 10, \"units\": 64},\n",
        "              {\"time_steps\": 10, \"units\": 128}]"
      ],
      "id": "3O-VlqD0oNZj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNaCMyLFoNZj"
      },
      "outputs": [],
      "source": [
        "seed = 200\n",
        "columns = \"w5_features\"\n",
        "params_idx = 1\n",
        "params = params_rnn[params_idx]\n",
        "\n",
        "model_lstm = keras.models.load_model(models_path + \"lstm\" + \"-\" + columns + \"-s\" + str(seed) + \"-p\" + str(params_idx))\n",
        "f = results_path + \"lstm-\" + columns + \"-s\" + str(seed) + \"-p\" + str(params_idx)\n",
        "with open(f, \"rb\") as file:\n",
        "    training_columns, _, _, best_cost_lstm, best_thr_lstm, _, _, _ = pickle.load(file)\n",
        "\n",
        "test_set_lstm = build_dataset_for_ml_model(df, training_columns=training_columns, split_size=1)\n",
        "X_test_lstm, y_test_lstm = create_dataset_3D(test_set_lstm[:, :-1],\n",
        "                                test_set_lstm[:, -1],\n",
        "                                params[\"time_steps\"])\n",
        "test_preds_lstm = model_lstm.predict(X_test_lstm).ravel()\n",
        "\n",
        "test_signal_lstm = pd.Series(data=(1 - test_preds_lstm))\n",
        "test_rul_lstm = y_test_lstm\n",
        "\n",
        "t = threshold_path + \"lstm-\" + columns + \"-s\" + str(seed) + \"-p\" + str(params_idx)\n",
        "to_serialize = test_signal_lstm, best_thr_lstm, test_rul_lstm\n",
        "with open(t, \"wb\") as file:\n",
        "    pickle.dump(to_serialize, file)\n",
        "\n",
        "test_perf_lstm = performance_evaluation(test_signal_lstm, best_thr_lstm, test_rul_lstm)\n",
        "all_perf.append([\"lstm\", seed, columns, params, params_idx] + test_perf_lstm)"
      ],
      "id": "VNaCMyLFoNZj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WE5CzM5UoNZk"
      },
      "source": [
        "## Recurrent Neural Network (GRU)"
      ],
      "id": "WE5CzM5UoNZk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKZfcrmQoNZk"
      },
      "outputs": [],
      "source": [
        "seed = 300\n",
        "columns = \"all_features\"\n",
        "params_idx = 0\n",
        "params = params_rnn[params_idx]\n",
        "\n",
        "model_gru = keras.models.load_model(models_path + \"gru\" + \"-\" + columns + \"-s\" + str(seed) + \"-p\" + str(params_idx))\n",
        "f = results_path + \"gru-\" + columns + \"-s\" + str(seed) + \"-p\" + str(params_idx)\n",
        "with open(f, \"rb\") as file:\n",
        "    training_columns, _, _, best_cost_gru, best_thr_gru, _, _, _ = pickle.load(file)\n",
        "\n",
        "test_set_gru = build_dataset_for_ml_model(df, training_columns=training_columns, split_size=1)\n",
        "X_test_gru, y_test_gru = create_dataset_3D(test_set_gru[:, :-1],\n",
        "                                test_set_gru[:, -1],\n",
        "                                params[\"time_steps\"])\n",
        "test_preds_gru = model_gru.predict(X_test_gru).ravel()\n",
        "\n",
        "test_signal_gru = pd.Series(data=(1 - test_preds_gru))\n",
        "test_rul_gru = y_test_gru\n",
        "\n",
        "t = threshold_path + \"gru-\" + columns + \"-s\" + str(seed) + \"-p\" + str(params_idx)\n",
        "to_serialize = test_signal_gru, best_thr_gru, test_rul_gru\n",
        "with open(t, \"wb\") as file:\n",
        "    pickle.dump(to_serialize, file)\n",
        "\n",
        "test_perf_gru = performance_evaluation(test_signal_gru, best_thr_gru, test_rul_gru)\n",
        "all_perf.append([\"gru\", seed, columns, params, params_idx] + test_perf_gru)"
      ],
      "id": "wKZfcrmQoNZk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFFl8hh7oNZl"
      },
      "source": [
        "## Recurrent Neural Network (BiLSTM)"
      ],
      "id": "yFFl8hh7oNZl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PG8lFFR1oNZl"
      },
      "outputs": [],
      "source": [
        "seed = 100\n",
        "columns = \"w15_features\"\n",
        "params_idx = 1\n",
        "params = params_rnn[params_idx]\n",
        "\n",
        "model_bilstm = keras.models.load_model(models_path + \"bilstm\" + \"-\" + columns + \"-s\" + str(seed) + \"-p\" + str(params_idx))\n",
        "f = results_path + \"bilstm-\" + columns + \"-s\" + str(seed) + \"-p\" + str(params_idx)\n",
        "with open(f, \"rb\") as file:\n",
        "    training_columns, _, _, best_cost_bilstm, best_thr_bilstm, _, _, _ = pickle.load(file)\n",
        "\n",
        "test_set_bilstm = build_dataset_for_ml_model(df, training_columns=training_columns, split_size=1)\n",
        "X_test_bilstm, y_test_bilstm = create_dataset_3D(test_set_bilstm[:, :-1],\n",
        "                                test_set_bilstm[:, -1],\n",
        "                                params[\"time_steps\"])\n",
        "test_preds_bilstm = model_bilstm.predict(X_test_bilstm).ravel()\n",
        "\n",
        "test_signal_bilstm = pd.Series(data=(1 - test_preds_bilstm))\n",
        "test_rul_bilstm = y_test_bilstm\n",
        "\n",
        "t = threshold_path + \"bilstm-\" + columns + \"-s\" + str(seed) + \"-p\" + str(params_idx)\n",
        "to_serialize = test_signal_bilstm, best_thr_bilstm, test_rul_bilstm\n",
        "with open(t, \"wb\") as file:\n",
        "    pickle.dump(to_serialize, file)\n",
        "\n",
        "test_perf_bilstm = performance_evaluation(test_signal_bilstm, best_thr_bilstm, test_rul_bilstm)\n",
        "all_perf.append([\"bilstm\", seed, columns, params, params_idx] + test_perf_bilstm)"
      ],
      "id": "PG8lFFR1oNZl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformers"
      ],
      "metadata": {
        "id": "CXPLvlZLsdZd"
      },
      "id": "CXPLvlZLsdZd"
    },
    {
      "cell_type": "code",
      "source": [
        "params_transformer = [{\"head_size\": 128, \"num_heads\": 2, \"ff_dim\": 2, \"num_transformer_blocks\": 2,\n",
        "                       \"mlp_units\": [64], \"mlp_dropout\": 0.3, \"dropout\": 0.25, \"w_len\": 5},\n",
        "                      {\"head_size\": 128, \"num_heads\": 2, \"ff_dim\": 2, \"num_transformer_blocks\": 2,\n",
        "                       \"mlp_units\": [64], \"mlp_dropout\": 0.3, \"dropout\": 0.25, \"w_len\": 10},\n",
        "                      {\"head_size\": 128, \"num_heads\": 2, \"ff_dim\": 2, \"num_transformer_blocks\": 2,\n",
        "                       \"mlp_units\": [64], \"mlp_dropout\": 0.3, \"dropout\": 0.25, \"w_len\": 15},\n",
        "                      {\"head_size\": 256, \"num_heads\": 4, \"ff_dim\": 4, \"num_transformer_blocks\": 4,\n",
        "                       \"mlp_units\": [128], \"mlp_dropout\": 0.4, \"dropout\": 0.25, \"w_len\": 5},\n",
        "                      {\"head_size\": 256, \"num_heads\": 4, \"ff_dim\": 4, \"num_transformer_blocks\": 4,\n",
        "                       \"mlp_units\": [128], \"mlp_dropout\": 0.4, \"dropout\": 0.25, \"w_len\": 10},\n",
        "                      {\"head_size\": 256, \"num_heads\": 4, \"ff_dim\": 4, \"num_transformer_blocks\": 4,\n",
        "                       \"mlp_units\": [128], \"mlp_dropout\": 0.4, \"dropout\": 0.25, \"w_len\": 15},\n",
        "                      {\"head_size\": 128, \"num_heads\": 4, \"ff_dim\": 2, \"num_transformer_blocks\": 4,\n",
        "                       \"mlp_units\": [128], \"mlp_dropout\": 0.4, \"dropout\": 0.25, \"w_len\": 10},\n",
        "                      {\"head_size\": 128, \"num_heads\": 4, \"ff_dim\": 2, \"num_transformer_blocks\": 2,\n",
        "                       \"mlp_units\": [64], \"mlp_dropout\": 0.3, \"dropout\": 0.25, \"w_len\": 10},\n",
        "                      {\"head_size\": 256, \"num_heads\": 2, \"ff_dim\": 3, \"num_transformer_blocks\": 4,\n",
        "                       \"mlp_units\": [128, 64], \"mlp_dropout\": 0.4, \"dropout\": 0.25, \"w_len\": 15},\n",
        "                      {\"head_size\": 256, \"num_heads\": 3, \"ff_dim\": 3, \"num_transformer_blocks\": 3,\n",
        "                       \"mlp_units\": [128], \"mlp_dropout\": 0.4, \"dropout\": 0.25, \"w_len\": 10},]"
      ],
      "metadata": {
        "id": "ffVMOBPNsgTg"
      },
      "id": "ffVMOBPNsgTg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 301\n",
        "columns = \"w15_features\"\n",
        "params_idx = 3\n",
        "params = params_transformer[params_idx]\n",
        "\n",
        "transformer = keras.models.load_model(models_path + \"transformer\" + \"-\" + columns + \"-s\" + str(seed) + \"-p\" + str(params_idx))\n",
        "f = results_path + \"transformer-\" + columns + \"-s\" + str(seed) + \"-p\" + str(params_idx)\n",
        "with open(f, \"rb\") as file:\n",
        "    training_columns, _, _, best_cost, best_thr, _, _, _ = pickle.load(file)\n",
        "\n",
        "test_set = build_dataset_for_ml_model(df, training_columns=training_columns, split_size=1, as_list=True)\n",
        "ts_sw, ts_sw_r = sliding_window_by_fault(test_set, training_columns.copy(), params[\"w_len\"])\n",
        "\n",
        "test_preds = transformer.predict(ts_sw).ravel()\n",
        "\n",
        "test_signal = pd.Series(data=(1 - test_preds))\n",
        "test_rul = ts_sw_r\n",
        "\n",
        "t = threshold_path + \"transformer-\" + columns + \"-s\" + str(seed) + \"-p\" + str(params_idx)\n",
        "to_serialize = test_signal, best_thr, test_rul\n",
        "with open(t, \"wb\") as file:\n",
        "    pickle.dump(to_serialize, file)\n",
        "\n",
        "test_perf = performance_evaluation(test_signal, best_thr, test_rul)\n",
        "all_perf.append([\"transformer\", seed, columns, params, params_idx] + test_perf)"
      ],
      "metadata": {
        "id": "075LRQYvsmBX"
      },
      "id": "075LRQYvsmBX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b823860f",
      "metadata": {
        "id": "b823860f"
      },
      "source": [
        "## Analysis over the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dede535a",
      "metadata": {
        "id": "dede535a"
      },
      "outputs": [],
      "source": [
        "df_res = pd.DataFrame(all_perf, columns=[\"model\", \"seed\", \"columns\", \"params\", \"params_idx\", \"cost\", \"anticipation\", \"detected_faults\", \"missed_faults\", \"false_alarms\"])\n",
        "df_res.to_csv(\"testing_summary_\" + str(margin) + \".csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZHego15oNZm"
      },
      "outputs": [],
      "source": [],
      "id": "jZHego15oNZm"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}